{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1cc2f9",
   "metadata": {},
   "source": [
    "# Investigate How Preprocessing Steps Change the Vocabulary and Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbadfe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\myacoubalex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\myacoubalex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d82af",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c2f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = fetch_20newsgroups(categories=['sci.space'], remove=('headers', 'footers', 'quotes'))\n",
    "docs = data.data[:100]\n",
    "\n",
    "# Setup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd0e55",
   "metadata": {},
   "source": [
    "## preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f2dbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_a(doc):\n",
    "    return doc\n",
    "\n",
    "def version_b(doc):\n",
    "    doc = re.sub(r'\\W+', ' ', doc)  # Remove non-alphanumeric characters\n",
    "    tokens = word_tokenize(doc)\n",
    "    return tokens\n",
    "\n",
    "def version_c(doc):\n",
    "    tokens = version_b(doc)\n",
    "    tokens = [w.lower() for w in tokens if w.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def version_d(doc):\n",
    "    tokens = version_c(doc)\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c9b20",
   "metadata": {},
   "source": [
    "## analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3802296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(docs, version_func):\n",
    "    processed = [version_func(doc) for doc in docs]\n",
    "    if isinstance(processed[0], str):  # Version A (raw text)\n",
    "        tokenized = [word_tokenize(doc) for doc in processed]\n",
    "    else:\n",
    "        tokenized = processed\n",
    "\n",
    "    flat_tokens = [token for doc in tokenized for token in doc]\n",
    "    vocab = set(flat_tokens)\n",
    "    top_words = Counter(flat_tokens).most_common(10)\n",
    "    avg_len = sum(len(doc) for doc in tokenized) / len(tokenized)\n",
    "\n",
    "    return {\n",
    "        \"vocab_size\": len(vocab),\n",
    "        \"top_words\": top_words,\n",
    "        \"avg_len\": avg_len\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6505ecd",
   "metadata": {},
   "source": [
    "## analysis our versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e69be316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Version A\n",
      "Vocabulary Size: 5554\n",
      "Top 10 Words: [(',', 1054), ('.', 963), ('the', 864), ('of', 455), ('to', 446), ('and', 434), ('a', 397), ('--', 316), (')', 315), ('(', 300)]\n",
      "Average Words per Document: 232.28\n",
      "\n",
      "Version B\n",
      "Vocabulary Size: 5386\n",
      "Top 10 Words: [('the', 866), ('of', 456), ('to', 449), ('and', 435), ('a', 397), ('in', 282), ('is', 226), ('for', 220), ('that', 164), ('I', 160)]\n",
      "Average Words per Document: 201.34\n",
      "\n",
      "Version C\n",
      "Vocabulary Size: 4103\n",
      "Top 10 Words: [('space', 144), ('nasa', 79), ('would', 66), ('one', 54), ('earth', 53), ('also', 43), ('shuttle', 39), ('spacecraft', 37), ('program', 37), ('time', 36)]\n",
      "Average Words per Document: 111.76\n",
      "\n",
      "Version D\n",
      "Vocabulary Size: 3137\n",
      "Top 10 Words: [('space', 144), ('nasa', 79), ('orbit', 70), ('would', 66), ('use', 61), ('one', 56), ('launch', 55), ('earth', 53), ('mission', 51), ('program', 44)]\n",
      "Average Words per Document: 111.76\n"
     ]
    }
   ],
   "source": [
    "# Run analysis for all versions\n",
    "results = {\n",
    "    \"Version A\": analyze(docs, version_a),\n",
    "    \"Version B\": analyze(docs, version_b),\n",
    "    \"Version C\": analyze(docs, version_c),\n",
    "    \"Version D\": analyze(docs, version_d)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for version, stats in results.items():\n",
    "    print(f\"\\n{version}\")\n",
    "    print(\"Vocabulary Size:\", stats['vocab_size'])\n",
    "    print(\"Top 10 Words:\", stats['top_words'])\n",
    "    print(\"Average Words per Document:\", round(stats['avg_len'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50668ad8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb30226",
   "metadata": {},
   "source": [
    "### 1. Which steps had the biggest effect on the vocabulary?\n",
    "The biggest effect on vocabulary size happened between Version B and Version C, where I applied lowercasing and removed stopwords.This makes sense because a lot of common but less meaningful words like \"the\", \"and\", \"to\", etc., were removed.\n",
    "\n",
    "### 2. Did some important terms get removed?\n",
    "Yes, it's possible. For example, the word \"would\" is still frequent in Version C and D, but some other useful contextual words might have been removed when I eliminated stopwords. Also, sin Version Dtemming  replaced different forms of words with their root form.\n",
    "\n",
    "### 3. Can too much preprocessing harm the meaning?\n",
    "In Version D, although stemming helped reduce the vocabulary to 3137, it may have oversimplified some words and removed useful variations. While cleaning the data is helpful for analysis, doing too much can reduce the richness and make it harder to understand the original context. So, preprocessing should be balanced based on the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603a510",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
