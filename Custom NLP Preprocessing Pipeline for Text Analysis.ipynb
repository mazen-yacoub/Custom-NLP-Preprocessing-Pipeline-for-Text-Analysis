{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bcf3ad",
   "metadata": {},
   "source": [
    "# Design and Apply a Custom Text Cleaning Pipeline for Two Contrasting Categories and Analysis them "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823576e0",
   "metadata": {},
   "source": [
    "## Data Aquesition (load data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3a4664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\myacoubalex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\myacoubalex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\myacoubalex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\myacoubalex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import main libraries and classes \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7885b383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "1177\n",
      "['comp.graphics', 'sci.space']\n",
      "[0 1 0 1 1]\n",
      "Have you considered the Apple Laserwriter IIg. We use it for all our B&W\n",
      "image printing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decide the categories and load dataset\n",
    "categories = ['comp.graphics','sci.space']\n",
    "data = fetch_20newsgroups(subset='train',\n",
    "                          categories=categories,\n",
    "                          remove=('headers', 'footers', 'quotes'),\n",
    "                          random_state = 2025)\n",
    "# data exploration\n",
    "print(type(data))             # bunch object (dictionary) : collection of plain text\n",
    "print(len(data.data))         # Number of documents\n",
    "print(data.target_names)      # our categories check\n",
    "print(data.target[:5])        # First 5 category labels (0 or 1)\n",
    "print(data.data[0][:500])     # First 500 characters of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1c4e6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "<class 'list'>\n",
      "['Have you considered the Apple Laserwriter IIg. We use it for all our B&W\\nimage printing.\\n', \"\\nI've got the 6.0 spec (obviously since I quoted it in my last posting). \\nMy gripe about TIFF is that it's far too complicated and nearly\\ninfinitely easier to write than to read, which I think hurts your\\nacceptance by anything that will need to read those images (e.g.,\\npaint programs).\\n\\nIn a nutshell, I don't think TIFF is salvageable unless the fat is\\ntrimmed significantly- and then it wouldn't be TIFF anymore.  They\\nkeep trying to cut it back, but it's late now.  Maybe they >will< fix it,\\nand change that magic number to signify the lack of compatibility. \\nThat would probably make me happy.\", '\\n\\nFirst, thanks to all who replied to my original question.']\n"
     ]
    }
   ],
   "source": [
    "# Filter and collect 100 documents per category\n",
    "graphics_docs = [doc for i, doc in enumerate(data.data) if data.target[i] == 0][:100]\n",
    "space_docs = [doc for i, doc in enumerate(data.data) if data.target[i] == 1][:100]\n",
    "\n",
    "print(len(graphics_docs))     \n",
    "print(len(space_docs))        \n",
    "print(type(graphics_docs))    \n",
    "print(graphics_docs[:3])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8eb0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning by Regex\n",
    "def clean_text(text):\n",
    "    # Remove punctuation, numbers, and symbols\n",
    "    cleaned = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return cleaned\n",
    "\n",
    "cleaned_space = [clean_text(doc) for doc in space_docs]\n",
    "cleaned_graphics = [clean_text(doc) for doc in graphics_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df44a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++\n",
      "++Once inflated the substance was no longer\n",
      "++needed since there is nothing to cause the balloon to collapse.\n",
      "++This inflatable structure could suffer multiple holes with no \n",
      "++disastrous deflation.\n",
      "+\n",
      "+preasure (and the internal preasure that was needed to maintain\n",
      "+a spherical shape against this resistance) caused them to\n",
      "+catastrophically deflated.  The large silvered shards\n",
      "+\n",
      "+The billboard should pop like a dime store balloon.\n",
      "\n",
      "No, you're wrong about this. Give me some time to get my references.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Once inflated the substance was no longer\n",
      "needed since there is nothing to cause the balloon to collapse\n",
      "This inflatable structure could suffer multiple holes with no \n",
      "disastrous deflation\n",
      "\n",
      "preasure and the internal preasure that was needed to maintain\n",
      "a spherical shape against this resistance caused them to\n",
      "catastrophically deflated  The large silvered shards\n",
      "\n",
      "The billboard should pop like a dime store balloon\n",
      "\n",
      "No youre wrong about this Give me some time to get my references\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chaek the cleanig\n",
    "print(space_docs[2])\n",
    "print(\"-\"*30)\n",
    "print(cleaned_space[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef655cba",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e8459",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d9685dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'European', 'Space', 'Agency', 'has', 'involvement', 'with', 'remote', 'earth', 'observation', 'and', 'I', 'presume', 'this', 'includes', 'surveillance', 'optical', 'etc', 'So', 'its', 'not', 'just', 'the', 'USUSSRex', 'who', 'are', 'in', 'the', 'game', 'But', 'what', 'is', 'the', 'game', 'What', 'can', 'be', 'done', 'with', 'space', 'observation', 'The', 'military', 'functions', 'of', 'missile', 'spotting', 'troop', 'spotting', 'etc', 'are', 'well', 'documented', 'but', 'what', 'about', 'anything', 'else', 'The', 'biggest', 'eg', 'I', 'can', 'think', 'of', 'is', 'to', 'get', 'a', 'metal', 'sensing', 'sat', 'over', 'a', 'paying', 'country', 'and', 'scan', 'their', 'territory', 'for', 'precious', 'metals', 'More', 'importantly', 'if', 'radar', 'can', 'spot', 'water', 'vapour', 'clouds', 'presumably', 'a', 'radar', 'based', 'sat', 'will', 'be', 'capable', 'of', 'spotting', 'riversopen', 'water', 'and', 'underground', 'water', 'from', 'space', 'This', 'would', 'be', 'a', 'positive', 'life', 'saver', 'for', 'African', 'or', 'other', 'drought', 'affected', 'countries', 'Implementing', 'a', 'clean', 'water', 'and', 'irrigation', 'program', 'would', 'be', 'of', 'imense', 'benifit', 'to', 'such', 'countries', 'and', 'should', 'cut', 'down', 'mortalities', 'considerably', 'So', 'how', 'about', 'it', 'Is', 'there', 'a', 'charity', 'or', 'government', 'agency', 'that', 'would', 'pay', 'for', 'a', 'third', 'world', 'country', 'to', 'have', 'their', 'minerals', 'and', 'water', 'deposits', 'mapped', 'Or', 'is', 'this', 'still', 'scifi', 'Mail', 'replies', 'would', 'be', 'great', 'Thought', 'for', 'the', 'day', 'Thermal', 'energy', 'needs', 'water', 'to', 'make', 'steam', 'so', 'sstick', 'it', 'in', 'the', 'ocean']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_docs(doc_list):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of documents using nltk.word_tokenize.\n",
    "    \n",
    "    Parameters:\n",
    "    - doc_list: List of cleaned text documents (strings)\n",
    "    \n",
    "    Returns:\n",
    "    - List of tokenized documents (lists of words)\n",
    "    \"\"\"\n",
    "    return [word_tokenize(doc) for doc in doc_list]\n",
    "\n",
    "space_tokens = tokenize_docs(cleaned_space)\n",
    "graphics_tokens = tokenize_docs(cleaned_graphics)\n",
    "\n",
    "print(space_tokens[0])  # First sci.space document tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f5c1f",
   "metadata": {},
   "source": [
    "###  Case Folding (to lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64d62c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'ironic']\n",
      "['how', 'ironic']\n"
     ]
    }
   ],
   "source": [
    "def lowercase_tokens(docs):\n",
    "    \"\"\"\n",
    "    Lowercases all tokens in a list of tokenized documents.\n",
    "\n",
    "    Parameters:\n",
    "    - docs: List of lists of tokens (e.g., [['NASA', 'launched'], ['Space', 'Station']])\n",
    "\n",
    "    Returns:\n",
    "    - List of lists with lowercase tokens\n",
    "    \"\"\"\n",
    "    return [[word.lower() for word in doc] for doc in docs]\n",
    "\n",
    "lower_space_tokens = lowercase_tokens(space_tokens)\n",
    "lower_graphics_tokens = lowercase_tokens(graphics_tokens)\n",
    "\n",
    "print(space_tokens[3])\n",
    "print(lower_space_tokens[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae5799",
   "metadata": {},
   "source": [
    "###  Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffb789d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pat', 'sez', 'yeah', 'but', 'a', 'windscreen', 'cut', 'down', 'most', 'of', 'it', 'canopies', 'ended', 'it', 'completely', 'of', 'course', 'the', 'environment', 'in', 'space', 'continues', 'to', 'suck', 'tommy', 'mac', 'tom', 'mcwilliams', 'wk', 'as', 'the', 'radius', 'of', 'vision', 'increases', 'tmibmclmsuedu', 'hm', 'the', 'circumference', 'of', 'mystery', 'grows']\n",
      "['pat', 'sez', 'yeah', 'windscreen', 'cut', 'canopies', 'ended', 'completely', 'course', 'environment', 'space', 'continues', 'suck', 'tommy', 'mac', 'tom', 'mcwilliams', 'wk', 'radius', 'vision', 'increases', 'tmibmclmsuedu', 'hm', 'circumference', 'mystery', 'grows']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a list of words (a single document).\n",
    "\n",
    "    Parameters:\n",
    "    - text: a list of words (tokens)\n",
    "\n",
    "    Returns:\n",
    "    - A list of words with stopwords removed\n",
    "    \"\"\"\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "filtered_space_tokens = [remove_stopwords(doc) for doc in lower_space_tokens]\n",
    "filtered_graphics_tokens = [remove_stopwords(doc) for doc in lower_graphics_tokens]\n",
    "\n",
    "print(lower_space_tokens[4])\n",
    "print(filtered_space_tokens[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd04798",
   "metadata": {},
   "source": [
    "### lemmatization (back words to their roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ca2302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens_list):\n",
    "    \"\"\"\n",
    "    Lemmatizes all words in each document.\n",
    "\n",
    "    Parameters:\n",
    "    - tokens_list: list of token lists (one list per document)\n",
    "\n",
    "    Returns:\n",
    "    - List of lemmatized token lists\n",
    "    \"\"\"\n",
    "    return [[lemmatizer.lemmatize(word) for word in doc] for doc in tokens_list]\n",
    "\n",
    "lemm_space_tokens = lemmatize_tokens(filtered_space_tokens)\n",
    "lemm_graphics_tokens = lemmatize_tokens(filtered_graphics_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3032f35",
   "metadata": {},
   "source": [
    "### Top Most Freq words in each category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6dee46a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in sci.space:\n",
      "space      160\n",
      "would       79\n",
      "nasa        77\n",
      "data        65\n",
      "system      64\n",
      "lunar       55\n",
      "also        54\n",
      "one         51\n",
      "shuttle     51\n",
      "mission     50\n",
      "dtype: int64\n",
      "\n",
      "Top words in comp.graphics:\n",
      "image      48\n",
      "would      40\n",
      "compass    35\n",
      "point      31\n",
      "system     29\n",
      "file       28\n",
      "opcols     28\n",
      "graphic    27\n",
      "thanks     27\n",
      "color      26\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def most_frequent_words(docs, top_n=10):\n",
    "    \"\"\"\n",
    "    Counts most frequent words in a list of tokenized documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - docs: List of token lists (e.g. [['nasa', 'space'], ['space', 'rocket']])\n",
    "    - top_n: Number of top frequent words to return\n",
    "    \n",
    "    Returns:\n",
    "    - Pandas Series of top N most frequent words\n",
    "    \"\"\"\n",
    "    all_words = [word for doc in docs for word in doc]\n",
    "    word_counts = Counter(all_words)\n",
    "    return pd.Series(word_counts).sort_values(ascending=False).head(top_n)\n",
    "\n",
    "\n",
    "print(\"Top words in sci.space:\")\n",
    "print(most_frequent_words(lemm_space_tokens))\n",
    "\n",
    "print(\"\\nTop words in comp.graphics:\")\n",
    "print(most_frequent_words(lemm_graphics_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91b0ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce102eed",
   "metadata": {},
   "source": [
    "### Do they use different vocabularies? \n",
    "- if we notice the most freq words in each category we will conclued that :They use different vocabularies focused on different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991da25",
   "metadata": {},
   "source": [
    "### Are there words that clearly define one category more than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73c3ce20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common words: {'would', 'system'}\n",
      "\n",
      "Unique sci.space words: {'space', 'shuttle', 'one', 'also', 'nasa', 'mission', 'lunar', 'data'}\n",
      "\n",
      "Unique comp.graphics words: {'thanks', 'image', 'color', 'graphic', 'compass', 'file', 'point', 'opcols'}\n"
     ]
    }
   ],
   "source": [
    "# Get sets of top words\n",
    "space_top = set(most_frequent_words(lemm_space_tokens).index)\n",
    "graphics_top = set(most_frequent_words(lemm_graphics_tokens).index)\n",
    "\n",
    "# comparison between them\n",
    "common_words = space_top & graphics_top\n",
    "unique_space = space_top - graphics_top\n",
    "unique_graphics = graphics_top - space_top\n",
    "\n",
    "print(\"Common words:\", common_words)\n",
    "print(\"\\nUnique sci.space words:\", unique_space)\n",
    "print(\"\\nUnique comp.graphics words:\", unique_graphics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb6a14",
   "metadata": {},
   "source": [
    "**Critical Reflection**\n",
    "\n",
    "- How does preprocessing help reveal the “essence” of each topic?\n",
    "\n",
    "Preprocessing plays a vital role in revealing the “essence” of each topic by cleaning and simplifying raw text into meaningful representations. By removing noise such as punctuation, stopwords, and casing inconsistencies, and applying techniques like lemmatization, we reduce redundancy and highlight the true semantic content. This makes topic-specific patterns and keywords more visible and interpretable.\n",
    "    \n",
    "    \n",
    "- What might go wrong if we skipped any of the steps?\n",
    "\n",
    "If any step is skipped, the analysis can become inaccurate or misleading. For example, without lowercasing, words like “NASA” and “nasa” would be treated as different, distorting frequency counts. Without lemmatization, similar forms like “running” and “ran” wouldn't be grouped, weakening the signal. If stopwords are not removed, common words like “the” or “is” may overwhelm more meaningful ones. Therefore, each step in the preprocessing pipeline is essential for enhancing clarity, consistency, and relevance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee73f14",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bc1c7",
   "metadata": {},
   "source": [
    "###  Modular Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# those are all dependences and helper functions , we will use them in our pipeline (we imported and created them before but i write them here for clearness)\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stopword set and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove punctuation, numbers, and special characters.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "def lowercase_tokens(tokenized_list):\n",
    "    \"\"\"Convert a list of tokens to lowercase (expects list of lists).\"\"\"\n",
    "    return [[word.lower() for word in doc] for doc in tokenized_list]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from a list of tokens.\"\"\"\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokenized_list):\n",
    "    \"\"\"Lemmatize each token (expects list of lists).\"\"\"\n",
    "    return [[lemmatizer.lemmatize(word) for word in doc] for doc in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "013f5f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline for a single document\n",
    "def preprocess_document(doc):\n",
    "    cleaned = clean_text(doc)\n",
    "    tokens = word_tokenize(cleaned)  # Use word_tokenize for a single doc\n",
    "    tokens = lowercase_tokens([tokens])[0]\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize_tokens([tokens])[0]\n",
    "    return tokens\n",
    "\n",
    "# Apply pipeline to a list of documents\n",
    "def preprocess_documents(docs):\n",
    "    return [preprocess_document(doc) for doc in docs]\n",
    "\n",
    "# Apply to both categories\n",
    "graphics_preprocessed = preprocess_documents(graphics_docs)\n",
    "space_preprocessed = preprocess_documents(space_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "31334e4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "comp.graphics sample: ['considered', 'apple', 'laserwriter', 'iig', 'use', 'bw', 'image', 'printing']\n",
      "comp.graphics sample: ['considered', 'apple', 'laserwriter', 'iig', 'use', 'bw', 'image', 'printing']\n"
     ]
    }
   ],
   "source": [
    "# check the length\n",
    "print(len(graphics_preprocessed)) \n",
    "\n",
    "# check the sample of doc from (cell by cell preprocessing was done above) Vs (pipeline)\n",
    "print(\"comp.graphics sample:\", graphics_preprocessed[0])\n",
    "print(\"comp.graphics sample:\", lemm_graphics_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf008f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
