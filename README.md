# Custom-NLP-Preprocessing-Pipeline-for-Text-Analysis
A complete Natural Language Processing (NLP) pipeline that cleans, tokenizes, normalizes, and analyzes text from two contrasting topics in the 20 Newsgroups dataset.

# ğŸ§¹ NLP Text Cleaning Pipeline: sci.space vs comp.graphics

This project implements a custom text preprocessing pipeline to clean and analyze raw text data from the [20 Newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) dataset using two contrasting categories:

- `sci.space`
- `comp.graphics`

## ğŸ” Objective

To evaluate how preprocessing transforms noisy, unstructured text into clean and meaningful input for analysis or machine learning â€” **even without training a model**.

---

## ğŸ§ª Preprocessing Pipeline

The custom pipeline includes:

1. âœ… **Regex Cleaning**  
   Remove punctuation, numbers, and symbols using `re`.

2. ğŸ§  **Tokenization**  
   Split documents into words using `nltk.word_tokenize()`.

3. ğŸ”¡ **Lowercasing**  
   Convert all tokens to lowercase.

4. ğŸš« **Stopword Removal**  
   Remove common English stopwords using NLTK.

5. ğŸ§¬ **Lemmatization**  
   Normalize words to their base form using `WordNetLemmatizer`.

---

## ğŸ“Š Analysis

After cleaning and preprocessing, the project:

- Extracts the **Top 10 most frequent words** for each category.
- Compares vocabularies between sci.space and comp.graphics.
- Identifies **unique vs. shared words** in both topics.

---

## ğŸ“‚ Folder Structure

```bash
ğŸ“ nlp-cleaning-pipeline/
â”‚
â”œâ”€â”€ nlp_preprocessing_pipeline.ipynb   # Jupyter Notebook with full code & analysis
â”œâ”€â”€ README.md                          # This file

## install Requirements
`pip install nltk scikit-learn pandas matplotlib seaborn`

## Download NLTK Resources
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')



